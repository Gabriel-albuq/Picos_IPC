{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OBS: No Roboflow ao misturar Retangulos e Poligonos, se exportar como Yolov8 pode apresentar erros, por isso farei manualmente a conversão de COCOMM para Yolo\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from matplotlib.patches import Rectangle\n",
    "import sys\n",
    "import torch\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import CocoDetection\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from datetime import datetime\n",
    "import onnx\n",
    "import onnxruntime as ort\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "base_dir = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "print(base_dir)\n",
    "\n",
    "sys.path.append(base_dir)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "\n",
    "# Diretório onde estão as imagens e as anotações\n",
    "images_dir = r'data/inputs/train_images/COCO_gx/train'  \n",
    "annotations_path = r'data/inputs/train_images/COCO_gx/train/_annotations.coco.json'  \n",
    "\n",
    "# Carregar anotações COCO\n",
    "with open(os.path.join(base_dir, annotations_path)) as f:\n",
    "    coco_data = json.load(f)\n",
    "\n",
    "# Criar um dicionário para mapear IDs de imagens para anotações\n",
    "annotations_dict = {}\n",
    "for annotation in coco_data['annotations']:\n",
    "    image_id = annotation['image_id']\n",
    "    if image_id not in annotations_dict:\n",
    "        annotations_dict[image_id] = []\n",
    "    annotations_dict[image_id].append(annotation)\n",
    "images_dict = {image['id']: image['file_name'] for image in coco_data['images']}\n",
    "\n",
    "list(images_dict.items())[:5]  # Pega as 5 primeiras entradas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Test Images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Single annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "\n",
    "# Função para carregar e visualizar imagens com uma anotação COCO específica\n",
    "def visualize_coco_single_annotation(image_path, annotations, image_name, annotation_index):\n",
    "    # Carregar a imagem\n",
    "    image = cv2.imread(image_path)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  # Converter BGR para RGB\n",
    "    \n",
    "    # Exibir o nome da imagem e as dimensões\n",
    "    height, width, channels = image.shape\n",
    "    print(f\"Exibindo imagem: {image_name}\")\n",
    "    print(f\"Dimensões da imagem: {width}x{height} (largura x altura, {channels} canais)\")\n",
    "    \n",
    "    # Criar uma figura para a imagem\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(image)\n",
    "\n",
    "    # Carregar apenas a anotação escolhida\n",
    "    if 0 <= annotation_index < len(annotations):\n",
    "        annotation = annotations[annotation_index]\n",
    "        x, y, w, h = annotation['bbox']  # bbox: [x, y, width, height]\n",
    "        \n",
    "        # Criar um retângulo para a caixa delimitadora\n",
    "        rect = Rectangle((x, y), w, h, linewidth=2, edgecolor='r', facecolor='none')\n",
    "        plt.gca().add_patch(rect)\n",
    "        \n",
    "        # Imprimir as coordenadas da bounding box\n",
    "        print(f\"Coordenadas da bounding box: x={x}, y={y}, width={w}, height={h}\")\n",
    "    else:\n",
    "        print(f\"Índice de anotação {annotation_index} não é válido. Existem apenas {len(annotations)} anotações.\")\n",
    "\n",
    "    plt.axis('off')  # Remover eixos\n",
    "    plt.show()  # Exibir a imagem com a marcação\n",
    "\n",
    "# Solicitar ao usuário para escolher uma imagem e uma anotação\n",
    "image_choice = 15  # Escolher a imagem (ajuste conforme necessário)\n",
    "annotation_choice = 0  # Escolher a anotação (ajuste conforme necessário)\n",
    "\n",
    "# Verificar se a escolha da imagem é válida\n",
    "if 0 <= image_choice < len(coco_data['images']):\n",
    "    image_info = coco_data['images'][image_choice]\n",
    "    image_file = image_info['file_name']\n",
    "    image_path = os.path.join(base_dir, images_dir, image_file)\n",
    "    image_id = image_info['id']\n",
    "    \n",
    "    # Obter anotações para a imagem selecionada\n",
    "    annotations = annotations_dict.get(image_id, [])\n",
    "    \n",
    "    # Visualizar a imagem com a anotação específica\n",
    "    visualize_coco_single_annotation(image_path, annotations, image_file, annotation_choice)\n",
    "else:\n",
    "    print(\"Escolha inválida!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 All annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "\n",
    "# Função para carregar e visualizar todas as anotações de uma imagem COCO\n",
    "def visualize_coco_all_annotations(image_path, annotations, image_name):\n",
    "    # Carregar a imagem\n",
    "    image = cv2.imread(image_path)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  # Converter BGR para RGB\n",
    "    \n",
    "    # Exibir o nome da imagem e as dimensões\n",
    "    height, width, channels = image.shape\n",
    "    print(f\"Exibindo imagem: {image_name}\")\n",
    "    print(f\"Dimensões da imagem: {width}x{height} (largura x altura, {channels} canais)\")\n",
    "    \n",
    "    # Criar uma figura para a imagem\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(image)\n",
    "\n",
    "    # Iterar sobre todas as anotações e desenhar as caixas delimitadoras\n",
    "    for annotation in annotations:\n",
    "        x, y, w, h = annotation['bbox']  # bbox: [x, y, width, height]\n",
    "        \n",
    "        # Criar um retângulo para a caixa delimitadora\n",
    "        rect = Rectangle((x, y), w, h, linewidth=2, edgecolor='r', facecolor='none')\n",
    "        plt.gca().add_patch(rect)\n",
    "        \n",
    "        # Imprimir as coordenadas da bounding box\n",
    "        print(f\"Coordenadas da bounding box: x={x}, y={y}, width={w}, height={h}\")\n",
    "\n",
    "    plt.axis('off')  # Remover eixos\n",
    "    plt.show()  # Exibir a imagem com as marcações\n",
    "\n",
    "# Solicitar ao usuário para escolher uma imagem\n",
    "image_choice = 15  # Escolher a imagem (ajuste conforme necessário)\n",
    "\n",
    "# Verificar se a escolha da imagem é válida\n",
    "if 0 <= image_choice < len(coco_data['images']):\n",
    "    image_info = coco_data['images'][image_choice]\n",
    "    image_file = image_info['file_name']\n",
    "    image_path = os.path.join(base_dir, images_dir, image_file)\n",
    "    image_id = image_info['id']\n",
    "    \n",
    "    # Obter anotações para a imagem selecionada\n",
    "    annotations = annotations_dict.get(image_id, [])\n",
    "    \n",
    "    # Visualizar a imagem com todas as anotações\n",
    "    visualize_coco_all_annotations(image_path, annotations, image_file)\n",
    "else:\n",
    "    print(\"Escolha inválida!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rw78W024zU83"
   },
   "source": [
    "# 4. Train Models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Funcoes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iniciar\n",
    "base_dir = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # Definir o dispositivo (GPU ou CPU)\n",
    "\n",
    "class CustomTransform: # Transformação das imagens\n",
    "    def __call__(self, img, target):\n",
    "        if isinstance(img, torch.Tensor):\n",
    "            return img.to(device), target  # Verifica se a imagem é tensor, caso seja já envia para o dispositivo\n",
    "\n",
    "        img = transforms.ToTensor()(img)  # Caso não seja Tensor, transforma em tensor\n",
    "        return img.to(device), target  # Envia a imagem para o dispositivo\n",
    "    \n",
    "class CustomDataset(CocoDetection): # Configuração do dataset personalizado\n",
    "    def __init__(self, root, annotation, transforms=None):\n",
    "        super().__init__(root, annotation)\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img, target = super().__getitem__(idx)\n",
    "\n",
    "        if self.transforms:\n",
    "            img, target = self.transforms(img, target)  # Aplica a transformação na imagem e no target\n",
    "\n",
    "        return img, target\n",
    "    \n",
    "def create_model(weights, num_classes):\n",
    "    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights=weights)\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    model.roi_heads.box_predictor = torchvision.models.detection.faster_rcnn.FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "    return model\n",
    "\n",
    "def load_pretrained_model(device, model, pretrained_model_path):\n",
    "    if pretrained_model_path and os.path.exists(pretrained_model_path):\n",
    "        model.load_state_dict(torch.load(pretrained_model_path, map_location=device))\n",
    "        print(f\"Modelo carregado de: {pretrained_model_path}\")\n",
    "\n",
    "    else:\n",
    "        print(\"Treinando modelo do zero...\")\n",
    "\n",
    "    model.to(device)\n",
    "    return model\n",
    "\n",
    "def dataset_config(images_dir, annotations_path, data_loader_batch_size):\n",
    "    dataset = CustomDataset(\n",
    "        root=os.path.join(base_dir, images_dir),\n",
    "        annotation=os.path.join(base_dir, annotations_path),\n",
    "        transforms=CustomTransform()  # Passa a transformação customizada\n",
    "    )\n",
    "\n",
    "    # Configuração do DataLoader\n",
    "    data_loader = DataLoader(dataset, batch_size=data_loader_batch_size, shuffle=True, collate_fn=lambda x: tuple(zip(*x)))\n",
    "\n",
    "    return dataset, data_loader\n",
    "\n",
    "def run_train(device, model, data_loader, optimizer, num_epochs, save_path):\n",
    "    best_loss = float('inf')  # Inicializa como infinito\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        total_boxes = 0  # Inicializa o contador de caixas\n",
    "        for images, targets in data_loader:\n",
    "            # Envie imagens para o dispositivo\n",
    "            images = [image.to(device) for image in images]  # As imagens já são tensores\n",
    "\n",
    "            # Processamento dos targets\n",
    "            processed_targets = []\n",
    "            for t in targets:\n",
    "                boxes = []\n",
    "                labels = []\n",
    "                for annotation in t:\n",
    "                    if 'bbox' in annotation and 'category_id' in annotation:  # Verifica se os campos existem\n",
    "                        bbox = annotation['bbox']  # COCO: [x_min, y_min, width, height]\n",
    "                        # Converte para [x_min, y_min, x_max, y_max]\n",
    "                        x_min = bbox[0]\n",
    "                        y_min = bbox[1]\n",
    "                        width = bbox[2]\n",
    "                        height = bbox[3]\n",
    "                        x_max = x_min + width\n",
    "                        y_max = y_min + height\n",
    "\n",
    "                        boxes.append([x_min, y_min, x_max, y_max])\n",
    "                        labels.append(annotation['category_id'])  # Adiciona o rótulo\n",
    "\n",
    "                if boxes:  # Se houver caixas, processa\n",
    "                    boxes_tensor = torch.tensor(boxes, dtype=torch.float32, device=device)  # Converte para tensor\n",
    "                    labels_tensor = torch.tensor(labels, dtype=torch.int64, device=device)  # Converte para tensor\n",
    "\n",
    "                    processed_targets.append({'boxes': boxes_tensor, 'labels': labels_tensor})\n",
    "                    total_boxes += len(boxes_tensor)  # Incrementa o contador de caixas\n",
    "                else:  # Para imagens sem anotações\n",
    "                    processed_targets.append({\n",
    "                        'boxes': torch.empty((0, 4), device=device),\n",
    "                        'labels': torch.empty((0,), dtype=torch.int64, device=device)\n",
    "                    })\n",
    "\n",
    "            # Calcule a perda somente se houver caixas em algum dos targets\n",
    "            if any(len(pt['boxes']) > 0 for pt in processed_targets):\n",
    "                loss_dict = model(images, processed_targets)\n",
    "                losses = sum(loss for loss in loss_dict.values())\n",
    "                total_loss += losses.item()\n",
    "\n",
    "                # Backpropagation\n",
    "                optimizer.zero_grad()\n",
    "                losses.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "        average_loss = total_loss / len(data_loader)\n",
    "        print(f\"Epoch {epoch + 1}, Loss: {average_loss:.4f}, Total Boxes: {total_boxes}\")\n",
    "\n",
    "        # Se o loss for menor que o melhor loss, salve o modelo\n",
    "        if average_loss < best_loss:\n",
    "            best_loss = average_loss\n",
    "            torch.save(model.state_dict(), save_path)\n",
    "            print(f\"Modelo salvo em: {save_path}\")\n",
    "\n",
    "def load_model_eval(device, model, pretrained_model_path):  # Colocar o modelo em modo de avaliação\n",
    "    model.load_state_dict(torch.load(pretrained_model_path))\n",
    "    model.to(device)\n",
    "    model.eval() \n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def load_image_tensor(image_path):\n",
    "    image = Image.open(image_path).convert(\"RGB\")  # Carrega a imagem e converte para RGB\n",
    "    transform = transforms.ToTensor()  # Define a transformação para ToTensor\n",
    "    image_tensor = transform(image).unsqueeze(0)  # Converte a imagem para tensor e adiciona uma dimensão de batch\n",
    "    \n",
    "    return image_tensor\n",
    "\n",
    "def visualize_predictions_image(device, model, image_tensor, threshold=0.5):\n",
    "    with torch.no_grad():  # Desativar cálculo de gradientes para economizar memória\n",
    "        image_tensor = image_tensor.to(device)  # Enviar imagem para o dispositivo\n",
    "        predictions = model(image_tensor)  # Fazer previsões\n",
    "\n",
    "    prediction = predictions[0]\n",
    "\n",
    "    image = image_tensor.squeeze(0).permute(1, 2, 0).cpu().numpy()  # Transpor a imagem para (H, W, C)\n",
    "\n",
    "    # Criar figura com duas subplots\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(16, 8))\n",
    "\n",
    "    # Exibir imagem original (sem marcações)\n",
    "    ax[0].imshow(image)\n",
    "    ax[0].axis('off')\n",
    "    ax[0].set_title('Imagem Original', fontsize=16)\n",
    "\n",
    "    # Exibir imagem com marcações\n",
    "    ax[1].imshow(image)\n",
    "    boxes = prediction['boxes'].cpu().detach().numpy()\n",
    "    scores = prediction['scores'].cpu().detach().numpy()\n",
    "\n",
    "    # Desenhar caixas delimitadoras\n",
    "    for box, score in zip(boxes, scores):\n",
    "        if score > threshold:  # Filtrar por pontuação\n",
    "            x_min, y_min, x_max, y_max = box\n",
    "            rect = patches.Rectangle((x_min, y_min), x_max - x_min, y_max - y_min,\n",
    "                                     linewidth=2, edgecolor='r', facecolor='none')\n",
    "            ax[1].add_patch(rect)\n",
    "\n",
    "    # Exibir o número total de detecções no topo da imagem com marcações\n",
    "    detections = (scores > threshold).sum()  # Contar detecções acima do limite\n",
    "    ax[1].set_title(f'Detecções Totais: {detections}', fontsize=16)\n",
    "    ax[1].axis('off')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "def load_onnx_model(device, model_onnx_path):\n",
    "    onnx_model = onnx.load(model_onnx_path)\n",
    "    onnx.checker.check_model(onnx_model) # Verificar se o modelo é válido\n",
    "    print(\"Modelo ONNX é válido!\")\n",
    "    \n",
    "    providers = ['CUDAExecutionProvider', 'CPUExecutionProvider'] if device == \"cuda\" else ['CPUExecutionProvider'] # Configuração do provedor (CUDA ou CPU)\n",
    "    ort_session = ort.InferenceSession(model_onnx_path, providers=providers) # Carregar o modelo ONNX com o provedor desejado\n",
    "    \n",
    "    return ort_session\n",
    "\n",
    "def visualize_predictions_onnx(device, ort_session, image_tensor, threshold=0.5):\n",
    "    image_tensor_device = image_tensor.to(device) # Enviar a imagem para o dispositivo (GPU ou CPU)\n",
    "    \n",
    "    input_name = ort_session.get_inputs()[0].name\n",
    "    outputs = ort_session.run(None, {input_name: image_tensor_device.cpu().numpy()})\n",
    "    \n",
    "    print(\"Inferência ONNX concluída com sucesso!\")\n",
    "    \n",
    "    # Carregar imagem\n",
    "    image = cv2.imread(image_path)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Extrair as saídas do modelo ONNX\n",
    "    boxes, labels, scores = outputs  # Ajuste aqui se os índices forem diferentes\n",
    "\n",
    "    # Converter para numpy array\n",
    "    boxes = np.array(boxes)\n",
    "    labels = np.array(labels)\n",
    "    scores = np.array(scores)\n",
    "\n",
    "    # Filtrar as detecções pelo threshold\n",
    "    for box, label, score in zip(boxes, labels, scores):\n",
    "        if score >= threshold:\n",
    "            x1, y1, x2, y2 = map(int, box)  # Coordenadas da caixa\n",
    "            cv2.rectangle(image, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "            text = f\"Classe {label} ({score:.2f})\"\n",
    "            cv2.putText(image, text, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "\n",
    "    # Exibir a imagem com as predições\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.imshow(image)\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Faster RCNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.1 FP32 (Padrão)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 102357,
     "status": "ok",
     "timestamp": 1730213828218,
     "user": {
      "displayName": "Gabriel Albuquerque",
      "userId": "09771626404184323064"
     },
     "user_tz": 180
    },
    "id": "dK--2gpDvKRe",
    "outputId": "4fbd3e02-d8ef-4294-c0b3-b68e570fd6cf"
   },
   "outputs": [],
   "source": [
    "# Iniciar\n",
    "base_dir = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # Definir o dispositivo (GPU ou CPU)\n",
    "\n",
    "# Inputs\n",
    "images_dir = r'data/inputs/train_images/COCO_gx/train' \n",
    "annotations_path = r'data/inputs/train_images/COCO_gx/train/_annotations.coco.json' \n",
    "save_dir = os.path.join(base_dir, 'data', 'inputs', 'ia_models', 'FRCNN Resnet50')\n",
    "models_path = os.path.join(base_dir, 'data', 'inputs', 'ia_models', 'FRCNN Resnet50')\n",
    "pretrained_model_path = os.path.join(models_path, 'best_faster_rcnn_model_20250326_154439.pth')  # Altere para o caminho do modelo salvo (ex: 'caminho/para/modelo.pth') ou mantenha None para treinar do zero\n",
    "weights='FasterRCNN_ResNet50_FPN_Weights.COCO_V1'\n",
    "num_classes = 2  # Inclua o número de classes (background + classes)\n",
    "data_loader_batch_size = 8\n",
    "num_epochs = 10000\n",
    "\n",
    "# Main\n",
    "model = create_model(weights, num_classes)\n",
    "model = load_pretrained_model(device, model, pretrained_model_path)\n",
    "dataset, data_loader = dataset_config(images_dir, annotations_path, data_loader_batch_size)\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-4, momentum=0.9, weight_decay=0.0005) # Defina o otimizador\n",
    "\n",
    "print(f\"Modelo está sendo treinado no dispositivo: {next(model.parameters()).device}\") # Verificar se o modelo está no dispositivo correto (GPU ou CPU)\n",
    "\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S') # Obter o timestamp atual para nomear o modelo\n",
    "save_path = os.path.join(save_dir,  f'best_faster_rcnn_model_{timestamp}.pth')\n",
    "\n",
    "run_train(device, model, data_loader, optimizer, num_epochs, save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.2 Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 782
    },
    "executionInfo": {
     "elapsed": 2553,
     "status": "ok",
     "timestamp": 1730213948077,
     "user": {
      "displayName": "Gabriel Albuquerque",
      "userId": "09771626404184323064"
     },
     "user_tz": 180
    },
    "id": "_t_LxBepw_AO",
    "outputId": "11d0d968-3541-48fa-fd11-dba1750e68bd"
   },
   "outputs": [],
   "source": [
    "# Iniciar\n",
    "base_dir = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # Definir o dispositivo (GPU ou CPU)\n",
    "\n",
    "# Inputs\n",
    "images_dir = r'data/inputs/train_images/COCO/train' \n",
    "annotations_path = r'data/inputs/train_images/COCO/train/_annotations.coco.json' \n",
    "save_dir = os.path.join(base_dir, 'data', 'inputs', 'ia_models', 'FRCNN Resnet50')\n",
    "models_path = os.path.join(base_dir, 'data', 'inputs', 'ia_models', 'FRCNN Resnet50')\n",
    "pretrained_model_path = os.path.join(models_path, 'best_faster_rcnn_model_20250326_154439.pth')  # Altere para o caminho do modelo salvo (ex: 'caminho/para/modelo.pth') ou mantenha None para treinar do zero\n",
    "weights='FasterRCNN_ResNet50_FPN_Weights.COCO_V1'\n",
    "num_classes = 2  # Inclua o número de classes (background + classes)\n",
    "num_epochs = 10000\n",
    "data_loader_batch_size = 1\n",
    "\n",
    "#  Main\n",
    "model = create_model(weights, num_classes)\n",
    "model = load_pretrained_model(device, model, pretrained_model_path)\n",
    "model = load_model_eval(device, model, pretrained_model_path)\n",
    "test_dataset, test_data_loader = dataset_config(images_dir, annotations_path, data_loader_batch_size)\n",
    "\n",
    "print(f\"Modelo no modo de avaliação no dispositivo: {next(model.parameters()).device}\") # Verificar se o modelo está no dispositivo correto (GPU ou CPU)\n",
    "\n",
    "# Caminho da imagem que você deseja testar\n",
    "image_path = r'/home/mdb/ProjetosPython/Picos_pip/data/inputs/test_images/Imagem0001 - 25.jpg'  # Substitua pelo caminho da sua imagem\n",
    "image_tensor = load_image_tensor(image_path)\n",
    "\n",
    "visualize_predictions_image(device, model, image_tensor, threshold=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 ONNX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kot-evfuzXQj"
   },
   "source": [
    "### 4.3.1 Convert to ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Hnut_76IyZJ8"
   },
   "outputs": [],
   "source": [
    "# Iniciar\n",
    "base_dir = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # Definir o dispositivo (GPU ou CPU)\n",
    "\n",
    "# Inputs\n",
    "images_dir = os.path.join(base_dir, r'data/inputs/train_images/COCO/train') \n",
    "annotations_path =  r'data/inputs/train_images/COCO/train/_annotations.coco.json'\n",
    "save_dir = os.path.join(base_dir, 'data', 'inputs', 'ia_models', 'FRCNN Resnet50')\n",
    "models_path = os.path.join(base_dir, 'data', 'inputs', 'ia_models', 'FRCNN Resnet50')\n",
    "pretrained_model_path = os.path.join(models_path, 'best_faster_rcnn_model_20250326_154439.pth')  # Altere para o caminho do modelo salvo (ex: 'caminho/para/modelo.pth') ou mantenha None para treinar do zero\n",
    "weights='FasterRCNN_ResNet50_FPN_Weights.COCO_V1'\n",
    "num_classes = 2  # Inclua o número de classes (background + classes)\n",
    "data_loader_batch_size = 8\n",
    "num_epochs = 10000\n",
    "\n",
    "model = create_model(weights, num_classes)\n",
    "model = load_pretrained_model(device, model, pretrained_model_path)\n",
    "model = load_model_eval(device, model, pretrained_model_path)\n",
    "dataset, data_loader = dataset_config(images_dir, annotations_path, data_loader_batch_size)\n",
    "\n",
    "dummy_input = torch.randn(1, 3, 640, 640, device=device) # Criar um tensor de entrada simulado (1 imagem com 3 canais, 640x640)\n",
    "\n",
    "model_onnx_path = os.path.join(models_path,  f'{os.path.splitext(os.path.basename(pretrained_model_path))[0]}.onnx') # Nome do arquivo ONNX\n",
    "\n",
    "# Exportar o modelo para ONNX\n",
    "torch.onnx.export(\n",
    "    model,                          # Modelo treinado\n",
    "    dummy_input,                    # Entrada simulada\n",
    "    model_onnx_path,                       # Nome do arquivo de saída\n",
    "    opset_version=11,                # Versão do conjunto de operações ONNX\n",
    "    input_names=[\"input\"],           # Nome da entrada\n",
    "    output_names=[\"boxes\", \"labels\", \"scores\"],  # Nomes das saídas\n",
    "    dynamic_axes={\"input\": {0: \"batch_size\"},   # Permite entradas de diferentes tamanhos\n",
    "                  \"boxes\": {0: \"batch_size\"},\n",
    "                  \"labels\": {0: \"batch_size\"},\n",
    "                  \"scores\": {0: \"batch_size\"}}\n",
    ")\n",
    "\n",
    "print(f\"Modelo convertido para ONNX e salvo como '{model_onnx_path}'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.2 Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iniciar\n",
    "base_dir = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # Definir o dispositivo (GPU ou CPU)\n",
    "\n",
    "# Inputs\n",
    "images_dir = os.path.join(base_dir, r'data/inputs/train_images/COCO/train') \n",
    "annotations_path =  r'data/inputs/train_images/COCO/train/_annotations.coco.json'\n",
    "save_dir = os.path.join(base_dir, 'data', 'inputs', 'ia_models', 'FRCNN Resnet50')\n",
    "models_path = os.path.join(base_dir, 'data', 'inputs', 'ia_models', 'FRCNN Resnet50')\n",
    "model_onnx_path = os.path.join(models_path, 'best_faster_rcnn_model_20250326_154439.onnx')  # Altere para o caminho do modelo salvo (ex: 'caminho/para/modelo.pth') ou mantenha None para treinar do zero\n",
    "weights='FasterRCNN_ResNet50_FPN_Weights.COCO_V1'\n",
    "num_classes = 2  # Inclua o número de classes (background + classes)\n",
    "data_loader_batch_size = 8\n",
    "num_epochs = 10000\n",
    "\n",
    "jpg_files = [f for f in os.listdir(images_dir) if f.lower().endswith('.jpg')]\n",
    "image_path = os.path.join(images_dir, jpg_files[0]) # Primeira Imagem\n",
    "image_tensor = load_image_tensor(image_path)\n",
    "ort_session = load_onnx_model(device, model_onnx_path)\n",
    "outputs = visualize_predictions_onnx(device, ort_session, image_tensor, threshold=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.4 Convert to TensorRT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inputs\n",
    "images_dir = os.path.join(base_dir, r'data/inputs/train_images/COCO/train') \n",
    "annotations_path =  r'data/inputs/train_images/COCO/train/_annotations.coco.json'\n",
    "save_dir = os.path.join(base_dir, 'data', 'inputs', 'ia_models', 'FRCNN Resnet50')\n",
    "models_path = os.path.join(base_dir, 'data', 'inputs', 'ia_models', 'FRCNN Resnet50')\n",
    "model_model_onnx_path = os.path.join(models_path, 'best_faster_rcnn_model_20250326_154439.onnx')  # Altere para o caminho do modelo salvo (ex: 'caminho/para/modelo.pth') ou mantenha None para treinar do zero\n",
    "weights='FasterRCNN_ResNet50_FPN_Weights.COCO_V1'\n",
    "num_classes = 2  # Inclua o número de classes (background + classes)\n",
    "data_loader_batch_size = 8\n",
    "num_epochs = 10000\n",
    "\n",
    "\n",
    "# Carregar o modelo ONNX\n",
    "onnx_model = onnx.load(model_onnx_path)\n",
    "\n",
    "# Verificar se há erros\n",
    "onnx.checker.check_model(onnx_model)\n",
    "print(\"Modelo ONNX é válido!\")\n",
    "\n",
    "# Testar inferência com ONNX Runtime\n",
    "ort_session = ort.InferenceSession(model_onnx_path)\n",
    "outputs = ort_session.run(None, {\"input\": dummy_input.cpu().numpy()})\n",
    "\n",
    "print(\"Inferência ONNX concluída com sucesso!\")\n",
    "\n",
    "# Converter ONNX para TensorRT\n",
    "TRT_LOGGER = trt.Logger(trt.Logger.WARNING)\n",
    "builder = trt.Builder(TRT_LOGGER)\n",
    "network = builder.create_network(1)\n",
    "parser = trt.OnnxParser(network, TRT_LOGGER)\n",
    "\n",
    "with open(model_onnx_path, 'rb') as model_file:\n",
    "    if not parser.parse(model_file.read()):\n",
    "        print(\"Falha ao analisar o modelo ONNX!\")\n",
    "        for error in range(parser.num_errors):\n",
    "            print(parser.get_error(error))\n",
    "        exit()\n",
    "\n",
    "config = builder.create_builder_config()\n",
    "config.max_workspace_size = 1 << 30  # 1GB\n",
    "engine = builder.build_engine(network, config)\n",
    "\n",
    "# Salvar o modelo TensorRT\n",
    "trt_engine_path = \"faster_rcnn_resnet50.trt\"\n",
    "with open(trt_engine_path, \"wb\") as f:\n",
    "    f.write(engine.serialize())\n",
    "\n",
    "print(f\"Modelo convertido para TensorRT e salvo como '{trt_engine_path}'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyPH7bsiQmRwS1J39N6wco6v",
   "gpuType": "T4",
   "name": "",
   "version": ""
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
